#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Patch pentru √ÆmbunƒÉtƒÉ»õirea downloader.py cu solu»õii concrete pentru toate platformele
"""

import os
import sys
import logging
import re
from datetime import datetime

# Configurare logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def apply_enhanced_downloader_patch():
    """AplicƒÉ patch-ul √ÆmbunƒÉtƒÉ»õit pentru downloader.py"""
    logger.info("üîß Aplicare patch √ÆmbunƒÉtƒÉ»õit pentru downloader.py...")
    
    downloader_path = 'downloader.py'
    
    if not os.path.exists(downloader_path):
        logger.error(f"‚ùå Fi»ôierul {downloader_path} nu existƒÉ!")
        return False
    
    # Cite»ôte con»õinutul actual
    with open(downloader_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Backup
    backup_path = f'downloader.py.backup_enhanced_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    with open(backup_path, 'w', encoding='utf-8') as f:
        f.write(content)
    logger.info(f"üìÑ Backup creat: {backup_path}")
    
    # 1. √émbunƒÉtƒÉ»õe»ôte configura»õiile yt-dlp pentru toate platformele
    enhanced_configs = '''
# Configura»õii √ÆmbunƒÉtƒÉ»õite pentru toate platformele
ENHANCED_PLATFORM_CONFIGS = {
    'tiktok': {
        'user_agents': [
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Android 14; Mobile; rv:121.0) Gecko/121.0 Firefox/121.0',
            'Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 5,
            'fragment_retries': 5,
            'extractor_retries': 3,
            'socket_timeout': 30,
            'sleep_interval': 2,
            'max_sleep_interval': 10,
            'geo_bypass': True,
            'nocheckcertificate': True,
            'extractor_args': {
                'tiktok': {
                    'webpage_download_timeout': 30,
                    'api_hostname': 'api.tiktokv.com'
                }
            }
        }
    },
    'instagram': {
        'user_agents': [
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 3,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True,
            'extractor_args': {
                'instagram': {
                    'api_version': 'v19.0',
                    'include_stories': False
                }
            }
        }
    },
    'reddit': {
        'user_agents': [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 5,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True
        }
    },
    'facebook': {
        'user_agents': [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 5,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True,
            'extractor_args': {
                'facebook': {
                    'api_version': 'v19.0',
                    'legacy_ssl': True,
                    'tab': 'videos'
                }
            }
        }
    },
    'twitter': {
        'user_agents': [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 3,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True
        }
    },
    'vimeo': {
        'user_agents': [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 3,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True
        }
    },
    'dailymotion': {
        'user_agents': [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 3,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True
        }
    },
    'pinterest': {
        'user_agents': [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 3,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True
        }
    },
    'threads': {
        'user_agents': [
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ],
        'ydl_opts_extra': {
            'http_chunk_size': 10485760,
            'retries': 3,
            'socket_timeout': 30,
            'geo_bypass': True,
            'nocheckcertificate': True,
            'extractor_args': {
                'instagram': {  # Threads folose»ôte Instagram extractor
                    'api_version': 'v19.0'
                }
            }
        }
    }
}

def get_platform_from_url(url):
    """DeterminƒÉ platforma din URL"""
    url_lower = url.lower()
    
    if any(domain in url_lower for domain in ['tiktok.com', 'vm.tiktok.com']):
        return 'tiktok'
    elif 'instagram.com' in url_lower:
        return 'instagram'
    elif any(domain in url_lower for domain in ['reddit.com', 'redd.it', 'v.redd.it']):
        return 'reddit'
    elif any(domain in url_lower for domain in ['facebook.com', 'fb.watch', 'fb.me']):
        return 'facebook'
    elif any(domain in url_lower for domain in ['twitter.com', 'x.com']):
        return 'twitter'
    elif 'vimeo.com' in url_lower:
        return 'vimeo'
    elif any(domain in url_lower for domain in ['dailymotion.com', 'dai.ly']):
        return 'dailymotion'
    elif any(domain in url_lower for domain in ['pinterest.com', 'pin.it']):
        return 'pinterest'
    elif 'threads.net' in url_lower:
        return 'threads'
    
    return 'generic'

def create_enhanced_ydl_opts(url, temp_dir):
    """CreeazƒÉ op»õiuni yt-dlp √ÆmbunƒÉtƒÉ»õite bazate pe platformƒÉ"""
    platform = get_platform_from_url(url)
    config = ENHANCED_PLATFORM_CONFIGS.get(platform, {})
    
    # Op»õiuni de bazƒÉ
    ydl_opts = {
        'format': 'best[filesize<45M][height<=720]/best[height<=720]/best',
        'outtmpl': os.path.join(temp_dir, '%(title)s.%(ext)s'),
        'restrictfilenames': True,
        'noplaylist': True,
        'extract_flat': False,
        'writethumbnail': False,
        'writeinfojson': False,
        'no_warnings': False,
        'ignoreerrors': False,
        'prefer_free_formats': False,
        'max_filesize': 50 * 1024 * 1024,  # 50MB
        'max_duration': 600,  # 10 minutes
        'http_headers': get_random_headers()
    }
    
    # AdaugƒÉ configura»õii specifice platformei
    if config.get('ydl_opts_extra'):
        ydl_opts.update(config['ydl_opts_extra'])
    
    # SeteazƒÉ user agent aleatoriu din lista platformei
    if config.get('user_agents'):
        import random
        user_agent = random.choice(config['user_agents'])
        ydl_opts['http_headers']['User-Agent'] = user_agent
    
    return ydl_opts

def download_with_enhanced_retry(url, temp_dir, max_attempts=3):
    """DescarcƒÉ cu strategii √ÆmbunƒÉtƒÉ»õite de retry"""
    platform = get_platform_from_url(url)
    config = ENHANCED_PLATFORM_CONFIGS.get(platform, {})
    
    last_error = None
    
    for attempt in range(max_attempts):
        try:
            logger.info(f"üîÑ √éncercare {attempt + 1}/{max_attempts} pentru {platform}...")
            
            # CreeazƒÉ op»õiuni √ÆmbunƒÉtƒÉ»õite
            ydl_opts = create_enhanced_ydl_opts(url, temp_dir)
            
            # AdaugƒÉ delay √Æntre √ÆncercƒÉri
            if attempt > 0:
                import time
                delay = 2 ** attempt  # Exponential backoff
                logger.info(f"‚è±Ô∏è A»ôteptare {delay}s √Ænainte de √Æncercarea {attempt + 1}...")
                time.sleep(delay)
            
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                # Extrage informa»õii
                info = ydl.extract_info(url, download=False)
                if not info:
                    raise Exception("Nu s-au putut extrage informa»õiile video")
                
                # VerificƒÉ dacƒÉ este live stream
                if info.get('is_live'):
                    raise Exception("Live stream-urile nu sunt suportate")
                
                # DescarcƒÉ videoclipul
                ydl.download([url])
                
                # GƒÉse»ôte fi»ôierul descƒÉrcat
                downloaded_files = []
                for file in os.listdir(temp_dir):
                    if file.endswith(('.mp4', '.mkv', '.webm', '.avi', '.mov', '.flv', '.3gp')):
                        downloaded_files.append(os.path.join(temp_dir, file))
                
                if not downloaded_files:
                    raise Exception("Nu s-a gƒÉsit fi»ôierul video descƒÉrcat")
                
                video_file = downloaded_files[0]
                file_size = os.path.getsize(video_file)
                
                logger.info(f"‚úÖ DescƒÉrcare reu»ôitƒÉ pentru {platform} la √Æncercarea {attempt + 1}")
                
                return {
                    'success': True,
                    'file_path': video_file,
                    'title': info.get('title', 'Video'),
                    'description': info.get('description', ''),
                    'uploader': info.get('uploader', ''),
                    'duration': info.get('duration', 0),
                    'file_size': file_size,
                    'platform': platform,
                    'attempt_number': attempt + 1
                }
                
        except Exception as e:
            error_msg = str(e)
            last_error = error_msg
            logger.warning(f"‚ùå √éncercarea {attempt + 1} e»ôuatƒÉ pentru {platform}: {error_msg[:100]}...")
            
            # VerificƒÉ dacƒÉ este o eroare criticƒÉ care nu meritƒÉ retry
            critical_errors = [
                'private video', 'video unavailable', 'video not found',
                'this video is private', 'content not available',
                'video has been removed', 'account suspended'
            ]
            
            if any(critical in error_msg.lower() for critical in critical_errors):
                logger.info(f"üõë Eroare criticƒÉ detectatƒÉ, oprire retry pentru {platform}")
                break
    
    return {
        'success': False,
        'error': f'‚ùå {platform.title()}: Toate √ÆncercƒÉrile au e»ôuat. Ultima eroare: {last_error}',
        'file_path': None,
        'file_size': 0,
        'duration': 0,
        'title': f'{platform.title()} - E»ôec descƒÉrcare',
        'platform': platform,
        'total_attempts': max_attempts
    }

'''
    
    # GƒÉse»ôte loca»õia unde sƒÉ insereze configura»õiile
    insert_pos = content.find('# Lista de User Agents reali')
    if insert_pos == -1:
        insert_pos = content.find('REAL_USER_AGENTS = [')
    
    if insert_pos != -1:
        content = content[:insert_pos] + enhanced_configs + '\n\n' + content[insert_pos:]
        logger.info("‚úÖ Configura»õii √ÆmbunƒÉtƒÉ»õite adƒÉugate")
    else:
        logger.warning("‚ö†Ô∏è Nu s-a gƒÉsit loca»õia pentru configura»õii, adƒÉugare la sf√¢r»ôitul fi»ôierului")
        content += '\n\n' + enhanced_configs
    
    # 2. √émbunƒÉtƒÉ»õe»ôte func»õia download_video
    download_function_pattern = r'(def download_video\([^)]+\):[^\n]*\n)(\s+)"""[^"]*"""\n'
    download_match = re.search(download_function_pattern, content, re.DOTALL)
    
    if download_match:
        indent = download_match.group(2)
        enhanced_download_start = f'''{download_match.group(1)}{download_match.group(2)}"""\n{indent}DescarcƒÉ un video cu strategii √ÆmbunƒÉtƒÉ»õite pentru toate platformele\n{indent}ReturneazƒÉ un dic»õionar cu rezultatul\n{indent}"""\n{indent}logger.info(f"=== ENHANCED DOWNLOAD_VIDEO START === URL: {{url}}")\n{indent}\n{indent}try:\n{indent}    # ValideazƒÉ URL-ul √Ænainte de procesare\n{indent}    logger.info(f"=== ENHANCED DOWNLOAD_VIDEO Validating URL ===")\n{indent}    is_valid, validation_msg = validate_url(url)\n{indent}    if not is_valid:\n{indent}        logger.error(f"=== ENHANCED DOWNLOAD_VIDEO URL Invalid === {{validation_msg}}")\n{indent}        return {{\n{indent}            'success': False,\n{indent}            'error': f'‚ùå URL invalid: {{validation_msg}}',\n{indent}            'title': 'N/A'\n{indent}        }}\n{indent}\n{indent}    # CreeazƒÉ directorul temporar\n{indent}    temp_dir = validate_and_create_temp_dir()\n{indent}    if not temp_dir:\n{indent}        return {{\n{indent}            'success': False,\n{indent}            'error': '‚ùå Nu s-a putut crea directorul temporar',\n{indent}            'title': 'N/A'\n{indent}        }}\n{indent}\n{indent}    logger.info(f"=== ENHANCED DOWNLOAD_VIDEO Temp dir created: {{temp_dir}} ===")\n{indent}\n{indent}    # Folose»ôte strategia √ÆmbunƒÉtƒÉ»õitƒÉ de descƒÉrcare\n{indent}    result = download_with_enhanced_retry(url, temp_dir, max_attempts=3)\n{indent}    \n{indent}    if result['success']:\n{indent}        logger.info(f"‚úÖ ENHANCED DOWNLOAD SUCCESS: {{result['title']}}")\n{indent}    else:\n{indent}        logger.error(f"‚ùå ENHANCED DOWNLOAD FAILED: {{result['error']}}")\n{indent}    \n{indent}    return result\n{indent}\n{indent}except Exception as e:\n{indent}    logger.error(f"=== ENHANCED DOWNLOAD_VIDEO Exception === {{str(e)}}")\n{indent}    import traceback\n{indent}    logger.error(f"=== ENHANCED DOWNLOAD_VIDEO Traceback === {{traceback.format_exc()}}")\n{indent}    return {{\n{indent}        'success': False,\n{indent}        'error': f'‚ùå Eroare nea»ôteptatƒÉ: {{str(e)}}',\n{indent}        'title': 'N/A'\n{indent}    }}\n{indent}\n{indent}finally:\n{indent}    # Nu »ôterge temp_dir aici - va fi »ôters dupƒÉ trimiterea fi»ôierului\n{indent}    pass\n\n'''
        
        # √énlocuie»ôte func»õia download_video existentƒÉ
        old_function_end = content.find('finally:', download_match.end())
        if old_function_end != -1:
            # GƒÉse»ôte sf√¢r»ôitul func»õiei
            lines = content[old_function_end:].split('\n')
            function_end = old_function_end
            for i, line in enumerate(lines):
                if line and not line.startswith(' ') and not line.startswith('\t') and i > 0:
                    function_end = old_function_end + len('\n'.join(lines[:i]))
                    break
            else:
                function_end = len(content)
            
            content = content[:download_match.start()] + enhanced_download_start + content[function_end:]
            logger.info("‚úÖ Func»õia download_video √ÆmbunƒÉtƒÉ»õitƒÉ")
        else:
            logger.warning("‚ö†Ô∏è Nu s-a putut gƒÉsi sf√¢r»ôitul func»õiei download_video")
    else:
        logger.warning("‚ö†Ô∏è Nu s-a gƒÉsit func»õia download_video")
    
    # 3. AdaugƒÉ func»õii de suport pentru URL variants
    url_variants_functions = '''
def get_url_variants(url):
    """GenereazƒÉ variante de URL pentru √ÆncercƒÉri multiple"""
    platform = get_platform_from_url(url)
    variants = [url]  # URL original
    
    if platform == 'facebook':
        # Variante Facebook
        if '/watch?v=' in url:
            video_id = url.split('/watch?v=')[1].split('&')[0]
            variants.extend([
                f"https://www.facebook.com/share/v/{video_id}/",
                f"https://m.facebook.com/watch?v={video_id}",
                f"https://fb.watch/{video_id}"
            ])
        elif '/share/v/' in url:
            video_id = url.split('/share/v/')[1].split('/')[0]
            variants.extend([
                f"https://www.facebook.com/watch?v={video_id}",
                f"https://m.facebook.com/watch?v={video_id}"
            ])
    
    elif platform == 'twitter':
        # Variante Twitter/X
        variants.extend([
            url.replace('twitter.com', 'x.com'),
            url.replace('x.com', 'twitter.com'),
            url.replace('mobile.twitter.com', 'twitter.com')
        ])
    
    elif platform == 'dailymotion':
        # Variante Dailymotion
        if 'dai.ly/' in url:
            video_id = url.split('dai.ly/')[1]
            variants.append(f"https://www.dailymotion.com/video/{video_id}")
        elif 'dailymotion.com/video/' in url:
            video_id = url.split('/video/')[1].split('?')[0]
            variants.append(f"https://dai.ly/{video_id}")
    
    elif platform == 'pinterest':
        # Variante Pinterest
        if 'pin.it/' in url:
            # Pentru pin.it, √ÆncercƒÉm sƒÉ gƒÉsim ID-ul real
            variants.append(url.replace('pin.it/', 'pinterest.com/pin/'))
    
    # EliminƒÉ duplicatele »ôi returneazƒÉ
    return list(dict.fromkeys(variants))

def normalize_url_for_platform(url):
    """NormalizeazƒÉ URL-ul pentru platformƒÉ"""
    platform = get_platform_from_url(url)
    
    if platform == 'reddit':
        # EliminƒÉ parametrii de query pentru Reddit
        url = url.split('?')[0].rstrip('/')
        # AsigurƒÉ-te cƒÉ nu se terminƒÉ cu .json
        if url.endswith('.json'):
            url = url[:-5]
    
    elif platform == 'tiktok':
        # NormalizeazƒÉ TikTok URLs
        if 'vm.tiktok.com' in url:
            # Pentru link-uri scurte, le lƒÉsƒÉm a»ôa cum sunt
            pass
        else:
            # EliminƒÉ parametrii extra
            url = url.split('?')[0]
    
    elif platform == 'instagram':
        # NormalizeazƒÉ Instagram URLs
        url = url.split('?')[0].rstrip('/')
    
    return url

'''
    
    # AdaugƒÉ func»õiile de suport
    content += '\n\n' + url_variants_functions
    logger.info("‚úÖ Func»õii de suport pentru URL variants adƒÉugate")
    
    # SalveazƒÉ fi»ôierul modificat
    with open(downloader_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info(f"‚úÖ Patch √ÆmbunƒÉtƒÉ»õit aplicat cu succes √Æn {downloader_path}")
    return True

def main():
    """Func»õia principalƒÉ"""
    logger.info(f"üöÄ √éncepere aplicare patch √ÆmbunƒÉtƒÉ»õit la {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        success = apply_enhanced_downloader_patch()
        
        if success:
            logger.info("\n" + "="*60)
            logger.info("‚úÖ PATCH √éMBUNƒÇTƒÇ»öIT APLICAT CU SUCCES!")
            logger.info("="*60)
            logger.info("\nüéØ √émbunƒÉtƒÉ»õiri implementate:")
            logger.info("  1. ‚úÖ Configura»õii optimizate pentru toate platformele")
            logger.info("  2. ‚úÖ Strategii de retry √ÆmbunƒÉtƒÉ»õite")
            logger.info("  3. ‚úÖ User-agent rotation per platformƒÉ")
            logger.info("  4. ‚úÖ Gestionare √ÆmbunƒÉtƒÉ»õitƒÉ a erorilor")
            logger.info("  5. ‚úÖ URL normalization »ôi variants")
            logger.info("  6. ‚úÖ Timeout-uri »ôi retry logic optimizate")
            logger.info("  7. ‚úÖ Geo-bypass »ôi SSL bypass pentru toate platformele")
            logger.info("\nüîÑ UrmƒÉtorii pa»ôi:")
            logger.info("  1. TesteazƒÉ din nou toate platformele")
            logger.info("  2. VerificƒÉ √ÆmbunƒÉtƒÉ»õirile √Æn ac»õiune")
            logger.info("  3. MonitorizeazƒÉ rata de succes")
            return 0
        else:
            logger.error("‚ùå Aplicarea patch-ului a e»ôuat")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Eroare √Æn timpul aplicƒÉrii patch-ului: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return 1

if __name__ == "__main__":
    sys.exit(main())